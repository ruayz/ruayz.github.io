---
title: "On the Fairness of Privacy Protection: Measuring and Mitigating the Disparity of Group Privacy Risk for Differentially Private Machine Learning"
collection: publications
category: conferences
permalink: /publication/2025-01-31-paper-title-number-3
excerpt: 'While privacy-aware and fair ML has advanced, the fairness of privacy protection remains underexplored. Compared to others, I defined a membership inference game to efficiently audit worst-case individual privacy risks, providing a more precise assessment of group privacy disparities. Furthermore, to address these disparities in DP-SGD, I developed a group-specific adaptive gradient clipping strategy, which reduces privacy risk imbalances while preserving DP guarantees, fostering the equitable deployment of AI systems.'
paperurl: 'http://ruayz.github.io/files/paper_icml.pdf'
---

Abstract: As machine learning (ML) algorithms are extensively adopted in various fields to make decisions of importance to human beings and our society, the fairness issue in algorithm decision-making has been widely studied. To mitigate unfairness in ML, many techniques have been proposed, including pre-processing, in-processing, and post-processing approaches. In this work, we propose an explainable feature selection (ExFS) method to improve the fairness of ML by recursively eliminating features that contribute to unfairness based on the feature attribution explanations of the model's predictions. To validate the effectiveness of our proposed ExFS method, we compare our approach with other fairness-aware feature selection methods on several commonly used datasets. The experimental results show that ExFS can effectively improve fairness by recursively dropping some features that contribute to unfairness. The ExFS method generally outperforms the compared filter-based feature selection methods in terms of fairness and achieves comparable results to the compared wrapper-based feature selection methods. In addition, our method can provide explanations for the rationale underlying this fairness-aware feature selection mechanism.  
