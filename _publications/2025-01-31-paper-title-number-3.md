---
title: "On the Fairness of Privacy Protection: Measuring and Mitigating the Disparity of Group Privacy Risk for Differentially Private Machine Learning"
collection: publications
category: conferences
permalink: /publication/2025-01-31-paper-title-number-3
venue: 'Manuscript'
excerpt: 'Summary: While fairness in conventional ML and differentially private ML (DPML) has been extensively studied, the fairness of privacy protection across groups remains underexplored. Existing methods assess group privacy risks using average-case measures, which may underestimate disparities. Worst-case auditing methods, though more accurate, are often computationally expensive.
To address these limitations, we propose a novel membership inference game that efficiently approximates worst-case privacy risks, enabling stricter and more reliable group risk assessments. To further enhance fairness, we introduce a group-specific adaptive gradient clipping strategy for DP-SGD, inspired by canary mechanisms in privacy auditing. Experiments demonstrate that our method significantly reduces disparities in group privacy risks, advancing fairness in DPML.'
paperurl: 'http://ruayz.github.io/files/paper_arxiv.pdf'
---

Abstract: Although research on privacy and fairness in machine learning (ML) has made significant progress, the fairness of privacy protection across different subgroups remains underexplored. Experimental results show that existing ML and differentially private machine learning (DPML) algorithms exhibit unfair privacy leakage risks among various groups, highlighting the need for further research in this area. Current studies primarily assess the average-case privacy risk of individual data records, which can overestimate protections for certain data points and underestimate disparities between groups. To address this, we introduce a membership inference game that efficiently audits the worst-case privacy risk associated with individual data points, allowing for more effective measurement of unfairness by evaluating privacy risk parity across groups. Based on our assessment of group privacy risk parity, we propose a novel technique to mitigate inequities in privacy protection for DPML. Inspired by canaries in differential privacy auditing, we enhance existing DPML algorithms with an adaptive group-specific gradient clipping strategy. Extensive experiments demonstrate that our approach maintains the same privacy guarantees as traditional DPML algorithms while effectively reducing disparities in group privacy risks, and promoting the ethical and equitable deployment of AI systems.

